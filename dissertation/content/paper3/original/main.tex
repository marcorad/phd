\documentclass[journal]{IEEEtran}

\usepackage{graphicx} % Required for inserting images
\usepackage{acronym}
\usepackage{mathtools} 
\usepackage{extarrows} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{upgreek}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{float}

\acrodef{1d}[1D]{1-dimensional}
\acrodef{2d}[2D]{2-dimensional}
\acrodef{cnn}[CNN]{convolutional neural network}
\acrodef{mfcc}[MFCC]{Mel-frequency cepstral coefficients}
\acrodef{sift}[SIFT]{scale-invariant feature transform}
\acrodef{ws}[WS]{wavelet scattering}
\acrodef{tf}[TF]{time-frequency}
\acrodef{lpf}[LPF]{low-pass filter}
\acrodef{gpu}[GPU]{graphics processing unit}
\acrodef{fir}[FIR]{finite impulse response}
\acrodef{sota}[SOTA]{state-of-the-art}
\acrodef{fft}[FFT]{fast Fourier transform}
\acrodef{lda}[LDA]{linear discriminant analysis}
\acrodef{nn}[NN]{neural network}
\acrodef{auc}[AUC]{area under curve}
\acrodef{std}[SD]{standard deviation}

\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Separable Wavelet Scattering}
\author{M.W. Rademan, D.J.J Versfeld, J.A. du Preez }
\date{November 2023}

\begin{document}

\maketitle

\begin{abstract}
Wavelet scattering is a widely used feature extraction method due to it efficacy in extracting invariant features, while retaining any lost high frequency information resulting from averaging to obtain the desired amount of invariance. By generalising a 1-dimensional scattering transform, we extend its definition to an arbitrary number of independent dimensions. We show that, in a modern deep learning setting, separable wavelet scattering performs similarly to its non-separable counterparts with the MNIST hand-written digit dataset. We additionally demonstrate state-of-the-art results for a subset of the MedMNIST3D datasets.
\end{abstract}

\begin{IEEEkeywords}
classification, MNIST, MedMNIST, wavelet scattering
\end{IEEEkeywords}

\section{Introduction}
\Ac{ws} has seen widespread use in classification applications as a powerful feature extraction method. It is an especially effective method for small datasets, since the feature filters are reminiscent of a \ac{cnn} front-end, while requiring no learning. The resulting features are invariant across all averaged dimensions, and exhibit separable class subspaces, allowing linear classifiers to be used with high efficacy \cite{2dscattering}.

Various forms of \ac{ws} exist, with \ac{1d} scattering first proposed by Anden and Mallat \cite{1dscattering1, ws}, which was later extended to 2 \cite{2dscattering} and 3 dimensions \cite{3dscattering, harmonicscattering}. Additional and more specialised forms for \ac{ws} include rotation-invariant scattering \cite{groupinvariantscattering} for the classification of textures and joint-\ac{tf} scattering \cite{ws_joint_tf, jointtfscattering2}. Joint-\ac{tf} scattering utilises a separable \ac{2d} filter that operates on the first level scattering scalogram, denoted by the operator $\mathcal{U}_1$, which is the only apparent usage of separable filters in the current scattering literature. 

In a deep-learning setting, sensible initialisation of filters prior to learning can significantly improve performance and interpretability \cite{sincnet}. The greatest advantage of separable filters is their computation speed when computing convolutions directly, as is performed in \acp{cnn} \cite{separablecnn}. Additionally, separable filters reduce the number of parameters of a \ac{cnn} if the filters are configured to be learnable. Learnable filters can typically improve performance compared to their fixed-filters counterpart \cite{scattering_birdsong}. 

At first glance, a \ac{ws} decomposition seems to be prohibitively expensive, but fast algorithms are possible due to the limited bandwidth of filter output. Fast algorithms utilise downsampling to take advantage of the demodulation of band-limited signals that result from the convolution of analytic wavelets \cite{2dscattering, 3dscattering, 1dscattering1}. Path pruning is also used to ignore filter combinations which have negligible energy. These algorithms are the standard implementation on many platforms, such as the Kymatio python package \cite{kymatio} and MATLAB \cite{MATLAB}.

The Morlet wavelet is the most widely used wavelet filter for a scattering filter bank implementation. Since the Morlet does not have compact support, implementations vary when considering the bandwidth/time support of the Morlet. This affects how Morlet \ac{fir} filters are discretised and truncated, how scattering paths with negligible energy are pruned, and how filters overlap in the frequency domain. 

In this work, we generalise \ac{1d} scattering to an arbitrary number of dimensions, which we refer to as the separable scattering transform. We propose flexible definitions for the Morlet bandwidth and filter overlap, which allows for an exact definition of near-optimal convolution computations with downsampling. Our implementation of separable scattering is \ac{gpu} accelerated, similar to the Kymatio implementations \cite{kymatio}. 

We demonstrate using the MNIST handwritten digit dataset \cite{mnist} that separable wavelet filters still perform adequately in a \ac{nn} classification setting compared to the conventional 2D scattering filters. We demonstrate the arbitrarily scalable dimensionality of the separable scattering transform with three-dimensional medical datasets from MedMNIST3D \cite{medmnist}. Separable scattering coefficients show \ac{sota} results for some of the MedMNIST3D datasets when combined with a simple \ac{nn}, while performing comparably to \ac{cnn} benchmarks on datasets which do not indicate \ac{sota} results.

\section{Separable Morlet Filterbank}

% In this section, we consider the continuous Morlet and provide general definitions of bandwidth and time-support for multiple dimensions. We utilise the Gaussian window to construct $n$-dimensional Morlets of varying characteristics.

\subsection{Morlet}

We define a \ac{1d} zero-mean Gaussian as
\begin{equation}
    \theta_{\sigma_t}(t) = \frac{1}{\sqrt{2\pi\sigma_t^2}}e^{-\frac{1}{2}\left(\frac{t}{\sigma_t}\right)^2},
\end{equation}
which has a Fourier transform transform $\theta_{\sigma_t}(t) \xleftrightarrow{\mathcal{F}} \hat{\theta}_{\sigma_\omega}(\omega)$, where $\sigma_\omega = \frac{1}{\sigma_t}$.

For an arbitrary bandwidth definition, we define the bandwidth-to-$\sigma$ ratio $\beta \in \mathbb{R}^+$, such that the one-sided bandwidth of a zero-mean Gaussian is $\beta \sigma_\omega$. For reference, Kymatio's implementation of 2D scattering indirectly defines $\beta \approx 2.5$ \cite{kymatio}.

% The $-3$ dB point of a Gaussian \ac{lpf} occurs at $\beta \approx 0.833$, but since the Gaussian \ac{lpf} does not have a very steep cutoff, values of $\beta \in [1, 3]$ may be appropriate to combat aliasing. 

A \ac{1d} Morlet $\uppsi$ has energy concentrated around 1 rad/s, with most of its energy contained in the interval $\omega \in [1 - \frac{1}{Q}, 1 + \frac{1}{Q}]$. $Q \in \mathbb{R}^+$ is defined as the number of wavelets per octave. The mother wavelet is given as
\begin{equation}
    \uppsi(t) = \theta_{\sigma_t}(t)\left(e^{jt} - \theta_0 \right),
\end{equation}
where $\theta_0 = \frac{\theta_{\sigma_t}(-1)}{\theta_{\sigma_t}(0)}$ to ensure zero mean: $\hat{\uppsi}(0) = 0$.

For reasons that will become apparent in section \ref{sec:filterbank}, we defined a 1D-wavelet dilated by a factor $\lambda$ as 
\begin{equation}
\label{eqn:dilwav}
    \uppsi_\lambda(t) = \begin{cases}
        \uppsi(\lambda t), \ \lambda \neq 0 \\
        \phi_j(t), \ \lambda = 0
    \end{cases},
\end{equation}
where $\phi$ represents the 1D \ac{lpf} utilised in scattering computations for the current dimension ($j$).

A $n$-dimensional separable wavelet may be constructed similarly, with
\begin{equation}
    \uppsi_{\vect{\lambda} }(\mathbf{u}) = \prod_{i=0}^{n-1} \uppsi_{\lambda_i}(u_i),
\end{equation}
where $\mathbf{u} = \left(u_1, ..., u_n\right)^T$ is the vector containing the dimensions of interest.

A filterbank is constructed by dilating the mother wavelet with a set of dilation factors $\vect{\lambda} =(\lambda_1, ..., \lambda_n )^T \in \Lambda_1 \times ... \times \Lambda_n$, with $\Lambda_i$ the set of dilation factors for the variable $u_i$. The dilation factor $\lambda_i$ is also the centre frequency of the dilated wavelet in rad/s. 

% Each dimension indexed by $i$ has its own defined \ac{lpf}.

% with with a dilated one-sided bandwidth of $\lambda_i \beta \sigma_{\omega_i}$. 

\subsection{1D Filterbank Construction}
\label{sec:filterbank}
 Suppose a 1D Morlet has a centre frequency $\lambda_0$. The following filter is placed at $\lambda_0 2^{\frac{1}{Q}}$. To define the amount of overlap between filters, it is useful express the corresponding filter's frequency \ac{std} $\sigma_\omega$ in terms of the distance between filters in the frequency domain. If the wavelet at $\lambda_0$ requires a decay equivalent to $\alpha \sigma_\omega$ \ac{std}s at the next wavelet at $\lambda_0 2^{\frac{1}{Q}}$. It follows that $\sigma_\omega = \frac{1}{\sigma_t} = \frac{1}{\alpha}\left( 2^\frac{1}{Q} - 1 \right)$.

We refer to $\alpha$ as the clearance factor. More overlap will result in more redundancy in the frequency representation, but also shorter filter impulse responses. 

% It is possible to set the clearance factor equal to the defined Gaussian bandwidth: $\alpha = \beta$. However, different values of $\alpha$ and $\beta$ allow for the separate definition of filter clearance and bandwidth, allowing the application to tune downsampling and filter time support separately. 

The provided definitions of filter clearance ($\alpha$) and bandwidth ($\beta$) may also be extended to non-separable wavelets, such as the rotational construction employed in \cite{2dscattering}. However, a conversion is required to find the bandwidth in each axis for a specific wavelet for rotationally constructed filterbanks, in order to employ the downsampling strategies proposed in this paper.

% A \ac{1d} wavelet filterbank is constructed with a set of filters $\mathbb{F} := \left\{\uppsi_\lambda \middle| \lambda \in \Lambda\right\}$ and a \ac{lpf} $\phi$ with the following specified parameters:
% \begin{enumerate}
%     \item $Q$ - wavelets per octave.
%     \item  $\omega_0$ - starting frequency in rad/s (optional).
%     \item  $d$ - invariance scale in samples, equivalent to the decimation factor applied to scattering coefficients.
% \end{enumerate}

All filterbank descriptions in this paper utilise normalised sampling frequency, i.e., the sample frequency is $f_s = 1$.

For some audio applications, for example speech recognition, low-frequency content is negligible, motivating filterbank construction starting at a specified frequency $\omega_0$. Otherwise, the entire frequency domain may be covered by setting $\omega_0$ to start at a position specified by $\phi$ and $\alpha$.

The \ac{lpf} $\phi$ is constant across all layers in a \ac{1d} scattering transform, and is chosen as $\phi(t) = \theta_{\sigma_{\phi, t}}(t)$, where $\sigma_{\phi, t} = \dfrac{d}{ \pi \beta}$.

The scattering transform requires that the time support of all filters do not exceed the time support of $\phi$, ensuring no filters contain time information exceeding the invariance scale $d$. $d$ is equivalently defined as the total downsample factor of the scattering tranform, and is unique for each dimension. Equivalently, the frequency \ac{std} of the filters may not exceed the \ac{std} of $\phi(\omega)$. For brevity, we denote the frequency \ac{std}s as $\sigma_{\lambda, \omega}$ and $\sigma_{\phi, \omega}$ for the Morlets and the \ac{lpf} respectively. 

 Since the maximum time support (minimum bandwidth) is a function of $d$, all dilated wavelets have their Gaussian envelopes restricted to a maximum time support of $\sigma_{\phi, t}$. This requires linearly spaced filters until the dilated bandwidth is larger than the \ac{lpf} bandwidth. To construct a set of positive dilation factors $\lambda \in \Lambda^+$, refer to algorithm \ref{alg:fb}.

\begin{algorithm}    
\caption{$\Lambda^+$ construction.}\label{alg:fb}
\begin{algorithmic}
    \State $\sigma_{\phi, \omega} \gets \dfrac{\pi\beta}{d}$
    \If{frequency limited}
        \Ensure{$\omega_0 \ge \dfrac{\pi\alpha}{d}$}
        \State $\lambda \gets \omega_0$
    \Else{}
        \State $\lambda \gets \dfrac{\pi\alpha}{d}$
    \EndIf
    \State $\Lambda^+ \gets \varnothing$
    \State $\sigma_\omega \gets \dfrac{1}{\alpha}\left( 2^\frac{1}{Q} - 1 \right)$
    \State $\sigma_{\lambda, \omega} \gets \lambda\sigma_\omega$
    
    \While{$\sigma_{\phi, \omega} > \sigma_{\lambda, \omega}$ and $\lambda < \pi$} 
        \State $\Lambda^+ \gets \Lambda^+ \cup \{\lambda\}$
        \State $\lambda \gets \lambda + \alpha \sigma_{\phi, \omega}$ 
        \State $\sigma_{\lambda, \omega} \gets \lambda\sigma_\omega$
    \EndWhile
    
    \While{$\lambda < \pi$}
        \State $\Lambda^+ \gets \Lambda^+ \cup \{\lambda\}$
        \State $\lambda \gets \lambda 2^{\frac{1}{Q}}$
        \State $\sigma_{\lambda, \omega} \gets \lambda\sigma_\omega$
    \EndWhile
\end{algorithmic}
\end{algorithm}

Only positive $\lambda$'s have been defined thus far, which provides inadequate coverage of the frequency domain in multiple dimensions. For real input signals, it is only necessary to cover half of one of the dimensions (only positive $\lambda$'s), whereas full coverage (both negative and positive $\lambda$'s) is required for additional dimensions. On-axis coverage is also required, in which each $\uppsi_\lambda$ must be multiplied with a Gaussian (zero-frequency wavelet), which the dilated wavelet definition in equation (\ref{eqn:dilwav}) defines as $\lambda=0$. A similar construction procedure is followed in \cite{jointtfscattering2}.

Given $m$ 1D filterbanks, with $m \ge 2$, each having Morlet filters with a positive set of lambdas $\Lambda_i^+$ and invariance scales $\vect{d} \in \mathbb{N}^m$, with $i = 1, ..., m$ indexing the dimension, we construct the $m$-dimensional filterbank with 
\begin{equation}
    \mathbb{F} = \left\{ \uppsi_{\vect{\lambda} }(\mathbf{u}) \ \middle| \ \vect{\lambda} \in \mathbb{L} \setminus \{\vect{0}\} \right\},
\end{equation}
where $\vect{u}$ is the $m$-dimensional spatial and/or time variable in which the each 1D filterbank is defined.  By definition, 
\begin{gather}
    \mathbb{L} = (\Lambda_1^+ \!\cup\! \{0\})\!\times\! (\Lambda_2^+ \!\cup\! \Lambda_2^- \!\cup \!\{0\}) \!\times\! ... \!\times\! (\Lambda_m^+ \!\cup\! \Lambda_m^- \!\cup\! \{0\}); \\
    \Lambda_i^- = \left\{-\lambda \ \middle| \ \lambda \in \Lambda_i^+\right\},
\end{gather}
where $\cup$ indicates the set union operator and $\times$ the Cartesian product. For $m=1$, the provided definitions result in a conventional \ac{1d} scattering transform \cite{1dscattering1}.

The $m$-dimensional \ac{lpf} is defined as 
\begin{equation}
    \phi(\vect{u}) = \uppsi_{\vect{0}}(\vect{u}).
\end{equation}

% The dilated wavelet definition provided by (\ref{eqn:dilwav}) therefore provides a concise set construction, with which to construct the \ac{lpf} and on-axis wavelets simultaneously.

% All on-axis filters will be highly correlated with their negative counterpart due to the symmetry of the multidimensional Fourier transform of real signals. For example, the 2D filter specified by $\lambda = [0, x]^T$ will be highly correlated with the filter at $[0, -x]^T$. As the invariance scale $d$ increases, the on-axis filters will become more narrow in the zero coordinate dimension, thereby becoming increasingly correlated.


% \subsection{Discretisation}

% Morlets do not have compact support, as such, they must be truncated for computation. The invariance scale $d$ specifies the maximum allowable time support of all filters. Since input signals must be at least of length $d$, filters can be precomputed using the padded signal length, while also guaranteeing a suitable level of truncation. The padding required is on the order of $d$ samples.

% Note that, due to their separability, all $\uppsi_{\vect{\lambda}} \in \mathbb{F}$ can be stored as $d$ one-dimensional filters, and not as $N_1 \times ... \times N_d$ tensors, thereby saving on the storage required for pre-computed filters. The savings on storage space allow implementations to compute the filters at multiple input sample frequencies, allowing for optimal downsampling strategies without incurring a significant storage cost.

\section{Separable Scattering Transform} \label{sec:wst}
% In this section, we describe the efficient computation of $m$-dimensional separable scattering transforms. The described downsampling strategy is utilised to a limited extent in Kymatio scattering implementations \cite{kymatio} and the original proposed fast algorithms \cite{2dscattering}, which is generalised further in this section.

\subsection{Transform}

The scattering transform requires 2 steps to provide scattering coefficients. The scalogram operator $\mathcal{U}_j$ iteratively filters a discrete signal $x[\vect{n}]$ for a given set of filters $\mathbb{F}$, which is then averaged by the \ac{lpf}. $\vect{n}$ represents a multidimensional index variable.
\begin{gather}
    \mathcal{U}_j x [\vect{n}, \vect{\lambda}_1, ..., \vect{\lambda}_j] = \left|\left(\mathcal{U}_{j-1} \ ...\  \mathcal{U}_1 x\right) * \uppsi_{\vect{\lambda}_j} \right|, \ \forall \ \uppsi_{\vect{\lambda}_j} \in \mathbb{F}; \\
     \mathcal{U}_1 x [\vect{n}, \vect{\lambda}_1] =  \left| x * \uppsi_{\vect{\lambda}_1} \right| , \ \uppsi_{\vect{\lambda}_1} \in \mathbb{F}.
\end{gather}

The scattering operator $\mathcal{S}_j$ provides the output coefficients at the $j$'th order of the scattering transform:
\begin{equation}
    \mathcal{S}_j x[\vect{n}, \vect{\lambda}_1, ..., \vect{\lambda}_j] = \mathcal{U}_j x * \phi.
\end{equation}
The \ac{lpf} $\phi$ remains constant throughout the transform. Note that the modulus/magnitude operator $|\cdot|$ demodulates the output of the filters, effectively extracting the Hilbert envelope from a band-limited signal \cite{waveletsandsubbandcoding}. 

The $j$'th scattering order adds an additional axis of paths indexed by $\vect{\lambda}_j$. However, not all paths need to be evaluated, since some paths have smaller bandwidths, thereby requiring fewer filters to extract the information lost by averaging. In particular, we only evaluate paths in which the centre frequencies of all elements of the vector $\vect{\lambda}_{j}$ are smaller than their corresponding bandwidth of the previous path's filter $\uppsi_{\vect{\lambda}_{j-1}}$. Path pruning is therefore dependent on $\alpha$ and $\beta$.

\subsection{Downsampling Strategy}
Since each filter specified by $\vect{\lambda}$ has its own bandwidth, we can employ downsampling across all paths non-uniformly. However, some care is required to ensure the compounded downsampling steps across all paths result in a uniform sampling frequency of the output scattering coefficients.

% The invariance scale $d$ is equivalent to the total downsampling required to compute scattering coefficients. $d$ is typically chosen as a power of $2$, which allows for multiple decimation stages to also subsample by powers of $2$. This decimation scheme is illustrated particularly well by dyadic wavelet transforms.

% However, it is not required that $d$ is restricted to a power of $2$, but rather that compounded decimation steps result in an effective decimation of $d$. As such, $d$ is most effective when it has as many factors as possible. In this study, we will require that $d$ is an even number.

Many applications are insensitive to small changes in $d$. As such, we propose a strategy to find an optimal $d$ given a target and tolerance value. For some applications, choosing $d$ such that the downsampling factor is a power of 2 is the simplest solution to achieve optimality. 

Without prior knowledge of the filterbank configurations, given a target invariance scale of $\bar{d}$ samples and a tolerance $\epsilon$, we can optimise $d \in \{\floor{(1 - \epsilon)\bar{d}}, \ceil{(1 + \epsilon)\bar{d}}\}$ such that it results in largest number of supported downsampling configurations.

A downsample factor $d$ which decomposes into a set of $n$ prime factors $\{p_1, ..., p_n\}$ with a corresponding multiset $\mathbb{M} = \left\{ m_1, ..., m_n \right\}$, where $m_i$ is the multiplicity of the prime $p_i$. We can find an optimal $d$ by maximising the sum $\sum\limits_{m \in M} m$.

% However, the proposed scheme for finding an optimal $d$ allows for more choices, thereby no restricting the choice of $d$ much, while also allowing for efficient dowmsampling.

Morlet filters in a 1D filterbank may be downsampled by a factor $d_{\psi_1}$ prior to low-pass filtering, and then downsampling again by a factor $d_{\phi_1}$ after low-pass filtering. As such, the compounded effect of downsampling restricts $d = d_{\psi_1} \cdot d_{\phi_1}$.

In the second order of scattering, the process is repeated with an additional pre-low-pass downsampling factor of $d_{\psi_2}$. The second level application of $\phi$ then downsamples by a factor $d_{\phi_2}$. To maintain a consistent output sampling frequency, it restricts $d = d_{\psi_2} \cdot d_{\psi_1} \cdot d_{\phi_2}$. 

Continuing the downsampling chain, the $i$'th level of downsampling requires $d = d_{\psi_i} \cdot ... \cdot d_{\psi_1} \cdot d_{\phi_i}$. The output of each operation of $\mathcal{U}_i$ and $\mathcal{S}_i$ must be downsampled as much as possible in order to make subsequent operations faster. To ensure that the application of all the downsampling steps are efficient, we require $d$ to have as many prime factors as possible, including factor multiplicity, so that a larger variety of downsampling combinations may be supported.



% \begin{algorithm}    
% \caption{Finding an optimal $d$.}\label{alg:optt}
% \begin{algorithmic}
%     \State $d_1 \gets \floor*{\frac{f_s \bar{T}(1 - \epsilon)}{2 \beta}}$\\
%     \State $d_2 \gets \ceil*{\frac{f_s \bar{T}(1 + \epsilon)}{2 \beta}}$\\
%     \State $k_\text{max} \gets 1$\\
%     \For{$d_j \in \{d_1, d_1 + 1, \ ... \ , d_2 - 1, d_2\}$}
%         \State $M \gets \text{PrimeMultiplicity}(d_j)$
%         \State  $k \gets \sum\limits_{m \in M} m$\\
%         \If{$k > k_\text{max}$}
%             \State $k_\text{max} \gets k$
%             \State $d \gets d_j$
%         \EndIf
%     \EndFor
% \end{algorithmic}
% \end{algorithm}


Consider a 1D wavelet filterbank and a single wavelet filter $\uppsi_1[n]$, applied to a discrete-time signal $x$. The operations required to compute the scattering coefficients is then notated for simplicity as
\begin{gather}
    u_1[n] = \left|  x * \uppsi_1   \right|; \\
    s_1[n] = y_1 * \phi.
\end{gather}

The bandwidth of $u_1$ is the bandwidth of an arbitrary first filter $\uppsi_1$. This follows from the Hilbert envelope computed by the analytic wavelet filter $\uppsi_\lambda$ and the modulus $|\cdot |$. The bandwidth of $s_1$ is the bandwidth of $\phi$.

Critical downsampling of a wavelet $\uppsi$ with a bandwidth of $\sigma_\omega$ is achieved by a factor of $d_{\uppsi} = \floor*{\frac{ \pi }{\beta\sigma_\omega}}$. Critical downsampling of $s_1$, is achieved via a factor of $d$, by definition.

We can efficiently compute $s_1$ using compounded downsampling steps:
\begin{equation}
    \left(s_1\right)_{\downarrow d} = \left( \left(u_1\right)_{\downarrow d_1} * (\phi)_{\downarrow d_1}\right)_{\downarrow d_2},
\end{equation}
such that $d = d_1 \cdot d_2, \ d_1, d_2 \in \mathbb{N}^+$, with $d_1 | d$ and $d_1 \le  d_{\uppsi_1}$. In order to find $d_1$, we decrement $d_{\uppsi_1}$ until it divides $d$ evenly. Each scalogram $u_j[n]$ is not necessarily downsampled optimally, but has a downsampling factor which guarentees a consistent scattering coefficient output sample frequency. 

% This strategy may counter intuitively result in faster computations for filterbanks with more filters, since a higher $Q$ implies a smaller bandwidth of each filter. 

A second order of scattering with a filter path of $(\uppsi_1, \uppsi_2)$ is performed on the downsampled $u_1$:
\begin{gather}
    u_2 = \left|  \left(u_1\right)_{\downarrow d_1} * (\uppsi_2)_{\downarrow d_1}  \right|; \\
    \left(s_2\right)_{\downarrow d} = \left( \left(u_2\right)_{\downarrow d_3} * (\phi)_{\downarrow d_3}\right)_{\downarrow d_4},
\end{gather}
such that $d = d_1 \cdot d_3 \cdot d_4, \ d_1, d_3, d_4 \in \mathbb{N}^+, d_3 \le d_{\uppsi_2}, d_3 | \frac{d}{d_1}$. The proposed downsampling scheme can be extended to an arbitrary number of scattering orders.

% Since the filterbank configuration is known, all required combinations of downsampled filters and their downsampling factors can be precomputed.

\begin{table*}[t!]
\centering
\caption{MedMNIST classification results of compared to baseline \ac{nn} approaches (\cite{medmnist})} \label{tab:medmnist}
\begin{tabular}{|l|rr|rr|rr|rr|rr|rr|}
\hline
\multirow{2}{*}{Methods}                & \multicolumn{2}{c}{Organ ($c=11$)}                  & \multicolumn{2}{c}{Nodule ($c=2$)}                 & \multicolumn{2}{c}{Fracture ($c=3$)}               & \multicolumn{2}{c}{Adrenal ($c=2$)}                & \multicolumn{2}{c}{Vessel ($c=2$)}                 & \multicolumn{2}{c|}{Synapse ($c=2$)}                \\
                         & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l|}{ACC} \\ \hline
ResNet-1810+2.5D        & 0.977                   & 0.788                   & 0.838                   & 0.835                   & 0.587                   & 0.451                   & 0.718                   & 0.772                   & 0.748                   & 0.846                   & 0.634                   & 0.696                   \\
ResNet-1810+3D          & 0.996                   & 0.907                   & 0.863                   & 0.844                   & 0.712                   & 0.508                   & 0.827                   & 0.721                   & 0.874                   & 0.877                   & 0.82                    & 0.745                   \\
ResNet-1810+ACS41       & 0.994                   & 0.900                     & 0.873                   & 0.847                   & 0.714                   & 0.497                   & 0.839                   & 0.754                   & 0.930                    & 0.928                   & 0.705                   & 0.722                   \\
ResNet-5010+2.5D        & 0.974                   & 0.769                   & 0.835                   & 0.848                   & 0.552                   & 0.397                   & 0.732                   & 0.763                   & 0.751                   & 0.877                   & 0.669                   & 0.735                   \\
ResNet-5010+3D          & 0.994                   & 0.883                   & 0.875                   & 0.847                   & 0.725                   & 0.494                   & 0.828                   & 0.745                   & 0.907                   & 0.918                   & \textbf{0.851}          & \textbf{0.795}          \\
ResNet-5010+ACS41       & 0.994                   & 0.889                   & 0.886                   & 0.841                   & \textbf{0.750}           & \textbf{0.517}          & 0.828                   & 0.758                   & 0.912                   & 0.858                   & 0.719                   & 0.709                   \\
auto-sklearn11          & 0.977                   & 0.814                   & \textbf{0.914}          & \textbf{0.874}          & 0.628                   & 0.453                   & 0.828                   & \textbf{0.802}          & 0.910                   & \textbf{0.915}          & 0.631                   & 0.730                    \\
AutoKeras12             & 0.979                   & 0.804                   & 0.844                   & 0.834                   & 0.642                   & 0.458                   & 0.804                   & 0.705                   & 0.773                   & 0.894                   & 0.538                   & 0.724                   \\ \hline
Separable WS + NN (Ours)  & \textbf{0.998}          & \textbf{0.941}          & 0.858                   & 0.797                   & 0.614                   & 0.458                   & \textbf{0.875}          & 0.792                   & \textbf{0.962}          & 0.895                   & 0.715                   & 0.525    \\ \hline              
\end{tabular}
\end{table*}

\subsection{Convolutions}
Optimal \ac{fft} convolutions can be achieved by performing downsampling in the frequency domain instead of the time domain. It is straightforward to verify that $|x * \uppsi|_{\downarrow r} = \left|(x * \uppsi)_{\downarrow r}\right|$, since the modulus is an element-wise operation, for some downsampling factor $r$. Given the signal and filter Fourier transforms $\hat{x}$ and $\hat{\uppsi}$, we then have
\begin{equation}
\label{eqn:circconv}
    x \otimes \uppsi [n] \xleftrightarrow{\mathcal{FFT}}  \hat{x} \cdot \hat{\uppsi} [k],
\end{equation}
where the $\otimes$ operator represents a circular convolution and $k$ is the frequency index.

Given that $r|N$, we can express (\ref{eqn:circconv}) when downsampled as a periodised summation \cite{waveletsandsubbandcoding} in the frequency domain
\begin{equation}
    (x \otimes \uppsi [n])_{\downarrow r} \xleftrightarrow{\mathcal{FFT}}  \frac{1}{r} \sum_{i=0}^{r-1} \hat{x} \cdot \hat{\uppsi} [k + iN/r], %https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5216ea733e562541b33a7f97dab0de072b2e8827
\end{equation}
which can be efficiently implemented via shape manipulation of tensors in computational packages like MATLAB or PyTorch \cite{pytorch}. To compute valid convolutions, we must pad $x$ and $\uppsi$ to have a total length of $N = N_x + d + c$, where $c \in \mathbb{N}^+$ is a constant that ensures that $d | N$. 

% Note that only $d$ additional samples are required instead of $2d$, since $d$ is, by definition, the time support of $\phi$ in number of samples. Padding is applied equally to the left and right of $x$. For further reduction of boundary effects, reflection padding is utilised. Kymatio's 2D scattering implementation utilises a similar padding scheme. 

% Thus far, the techniques discussed may be equivalently implemented on many scattering transforms. Separable scattering benefits from reduced computations resulting from filter separability. In particular, given a $m$-dimensional signal $x[\vect{n}]$ indexed by $\vect{n} = (n_1, ..., n_m)^T$, with each dimension having downsample factors expressed as a vector $\vect{r} = (r_1, ..., r_m)^T$, we can chain downsampling for each dimension:
% \begin{equation}
%     \left(x * \uppsi_{\vect{\lambda}}[\vect{n}]\right )_{\downarrow \vect{r}} = \left( \left(x[\vect{\lambda}] * \uppsi_{\lambda_i}[n_i]\right)_{\downarrow r_1} ... * \uppsi_{\lambda_m}[n_m]\right)_{\downarrow r_m}.
% \end{equation}.

% We abuse notation to indicate that $\downarrow r_i$ is an operator that downsamples the $i$'th dimension, and $\downarrow \vect{r}$ downsamples all dimensions each with their correponding downsample factor in $\vect{r} \in \mathbb{N}^m$.

% For $m$-dimensional sequences of dimensional lengths $N_1, ..., N_m$, the forward and inverse \acp{fft} computation time is reduced from $2\sum_{i} T \log N_i$ (no frequency periodisation), with $T = \prod_i N_i$ to $\sum_{i} \frac{T}{K_i} \log \frac{N_i}{K_i} + P \log P$, where $K_1 = 1$, $K_i = \prod_{j < i} r_j$ and $P = \prod_i \frac{N_i}{r_i}$. However, the number of convolution multiplications in the frequency domain increases from $\prod_i N_i$ to $\sum_i \prod_j \frac{N_j}{K_i} \leq m \prod_i N_i$.

% In contrast, optimal non-separable scattering transforms can achieve a computational cost of $\sum_{i} N_i \log N_i + P \log P$, since a full forward \ac{fft} is required to compute a convolution with a non-separable $m$-dimensional filter.

% The greatest computational advantage is during direct convolution computations, as is present in \acp{cnn}. For an $m$-dimensional filter impulse response with filter lengths $L_1, ..., L_m$, the computational cost reduces from $P\prod_i L_i$ to $P \sum_i L_i$.


\section{Results}



\subsection{MNIST}

% We benchmark separable \ac{ws} against 2D scattering by repeating the MNIST hand-written digit dataset experiment in \cite{2dscattering}. 

The MNIST dataset \cite{mnist} contains 60000 training and 10000 test samples. Unlike in \cite{2dscattering}, which decorrelates scattering coefficients with a discrete cosine transform prior to classification, we perform classification on the scattering coefficients directly. Kymatio \cite{kymatio} is used to produce the 2D scattering coefficients. Our implementation of separable scattering is implemented similarly to Kymatio, with PyTorch \cite{pytorch} as a backend for FFT convolutions and \ac{nn} models.

Unless specified otherwise, all experiments have $\beta = \alpha = 2.5$. Scattering features are normalised prior to classification, according to the mean and \ac{std} calculated on the training set. No data augmentation is performed.

\begin{table}
\centering
\caption{MNIST classification error rate (\%) of separable and \ac{2d} scattering coefficients using a \ac{nn} classifier} \label{tab:mnistnn}
\begin{tabular}{|lr|l|} \hline
\multirow{2}{*}{2D WS + NN}        & $l=1$, $J=2$  & $0.64 \pm 0.05$    \\
          & $l=2$, $J=3$ & $0.50 \pm 0.03$     \\ \hline
\multirow{2}{*}{Separable WS + NN}  & $l=1$, $d=(4,4)$ & $0.63 \pm 0.05 $    \\
          & $l=2$, $d=(4,4)$  & $0.52 \pm 0.04$     \\ \hline
\end{tabular}
\end{table}


% Table \ref{tab:mnistlda} shows the results for MNIST using a \ac{lda} classifier from the Scikit-learn python package \cite{sklearn}, with a covariance shrinkage factor of $5\cdot 10^{-3}$. Scattering is performed for $l=1, 2$ levels.

% \begin{table}[!h] 
% \centering
% \caption{MNIST classification error rate (\%) of separable and \ac{2d} scattering coefficients  using an \ac{lda} classifier} \label{tab:mnistlda}
% \begin{tabular}{|r|rr|rr|} \hline
% \multirow{2}{*}{Training size}              & \multicolumn{2}{l}{2D}                               & \multicolumn{2}{l|}{Separable}                             \\
%  & \multicolumn{1}{l}{$l=1$} & \multicolumn{1}{l}{$l=2$} & \multicolumn{1}{l}{$l=1$} & \multicolumn{1}{l|}{$l=2$} \\ \hline
% 1000                              & 4.58                        & 2.69                        & 4.87                        & 4.58                        \\
% 2000                              & 3.89                        & 1.62                        & 4.31                        & 2.6                         \\
% 5000                              & 3.54                        & 1.12                        & 3.84                        & 1.7                         \\
% 10000                             & 3.35                        & 1.05                        & 3.54                        & 1.5                         \\
% 20000                             & 3.16                        & 0.87                        & 3.41                        & 1.41                        \\
% 40000                             & 2.92                        & 0.86                        & 3.34                        & 1.25                        \\
% 60000                             & 2.92                        & 0.81                        & 3.27                        & 1.34     \\ \hline                  
% \end{tabular}
% \end{table}

Due to its separability and non-angularly spaced filters, separable scattering does not perform as well compared to 2D scattering when using simple classifiers, such as \ac{lda} \cite{lda}. To illustrate that this performance discrepancy is not of significant consequence in a \ac{nn} setting, we test performance on the full dataset utilising a simple architecture. The neural network architecture used has an input layer with 256 neurons, followed by two hidden layers with 128 and 64 neurons respectively. The output layer has 10 neurons - one for each digit. Input and hidden layers are followed by a batch norm layer \cite{batchnorm} and ReLU activation function \cite{relu}. The output layer is followed by a softmax function. The Adam optimiser \cite{adam} is used with cross-entropy loss, a batch size of 256 and learning rate of $3 \cdot 10^{-5}$. 5000 of the 60000 training samples are reserved for validation and removed from the training set. Training is stopped when validation loss starts to increase. The \ac{nn} model is initialised with random weights, and the experiment is repeated 50 times. Different invariance scales were tested, and the best results are reported in table \ref{tab:mnistnn}. Tests are repeated for $l \in \{1, 2\}$ levels of scattering.





\subsection{MedMNIST3D}






The MedMNIST3D datasets are a subset of the MedMNIST dataset group \cite{medmnist}, where each 3D dataset contains $28\times 28 \times 28$ images with 2, 3 or 11 classes. Train, test and validation data partitions are provided by the authors. All datasets have on the order of 1000 training samples. We compare the baseline \ac{nn} results provided in \cite{medmnist} with separable scattering features combined with a simple \ac{nn} classifier.

We use \ac{nn} classifier with an input layer containing 1024 neurons, followed by two hidden layers with 512 and 256 neurons respectively. Input and hidden layers layers are each followed by a batch norm layer \cite{batchnorm} and a ReLU non-linearity \cite{relu}. For datasets with two classes, the output layer is a single neuron followed by a sigmoid activation function. For datasets with more than two classes, the output layer has a size equal to the number of classes, followed by a softmax activation. Binary cross-entropy loss are used for datasets with two classes, otherwise cross-entropy loss is used. The Adam optimiser \cite{adam} with a learning rate of $1 \cdot 10^{-5}$ is used. All other configuration parameters are identical to the model used for the MNIST dataset.

A single level of scattering coefficients are computed, with $Q = (2, 2, 2)$ and $d = (4, 4, 4)$. Many MedMNIST3D datasets tend to be unbalanced, implying that \ac{auc} is a more reliable metric to measure model performance. The results are shown in table \ref{tab:medmnist}, in which \ac{sota} \ac{auc} performance is achieved for the Organ, Adrenal and Vessel datasets. Table \ref{tab:medmnist} indicates the number of classes ($c$) for each of the datasets, with accuracy (ACC) also shown for reference. The performance of our method on non-\ac{sota} results are comparable to the other baseline \ac{nn} approaches presented in \cite{medmnist}. It is likely that better results can be achieved by the proposed method if the filters are made learnable and/or scattering parameters are uniquely optimised for each dataset.


% \section{Conclusion}


\newpage
\bibliographystyle{ieeetr} 
\bibliography{refs} 

\end{document}
